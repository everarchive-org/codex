# 2.5.38 The R1 University Journey: "The Compliance Machine"

**Persona:** "Dr. Chen," the Digital Preservation Librarian at a major R1 research university, leading a team of four.
**Work:** Managing the complete digital lifecycle for "The Helios Project," a five-year, multi-million dollar solar energy research initiative funded by the National Science Foundation (NSF).

---

### **Primitives & Technologies Demonstrated**

*   **Ingestion Primitives (Automation & Intelligence):**
    *   `Template-Based Object Creation` (using the "NSF Research Project" template)
    *   `Policy-Based Ingestion`
    *   `Automated Instrument Data Ingestion`
    *   `Development Tool Integration` (Git)
    *   `Automated "Good Enough" QC`
    *   `AI-Assisted Metadata Extraction`
*   **Core Concepts:**
    *   `Reproducible Research`
    *   `Immutable Timestamping`
    *   `Verifiable intellectual lineage`
*   **Schema Projector Lenses:**
    *   `NSF DMSP Report Generation`
    *   `TRAC/ISO 16363 Audit Trail`
    *   `Reproducible Paper`
    *   `Live Project Dashboard`
*   **Licensing & Economic Primitives:**
    *   `Tiered Access Licensing` (for public vs. internal data)
*   **Standards & Compliance:**
    *   `NSF DMSP Compliance`
    *   `OAIS Model Alignment`
    *   `TRAC/ISO 16363 Audit Trail`

---

### **Part 1: The Ingestion Experience - From World to Archive**

#### **The Curation Narrative: Drowning in Data and Mandates**
Dr. Chen's world is one of scale and compliance. The Helios Project generates terabytes of data: satellite imagery, sensor readings, simulation outputs, software code, and dozens of papers. His primary mandate from the university is to ensure every byte is preserved in a way that satisfies their NSF grant's strict Data Management and Sharing Plan (DMSP) and the university's own goal of achieving TRAC certification for its repository. His team is skilled but small, and the manual process of ingesting, describing, and verifying this data stream is a constant, overwhelming bottleneck. He doesn't need a better storage system; he needs a "compliance machine."

**The Ingestion Process: The Automated, Policy-Driven Workflow**
Dr. Chen's team uses EverArchive to manage the entire project from day one.

1.  **Template & Policy:** At the project's outset, Dr. Chen creates a new Deep Authorship Package using the university's pre-defined **"NSF Research Project" Template**. This template automatically creates a specific folder structure (`/raw_data`, `/code`, `/manuscripts`, etc.) and applies a **Policy**:
    *   "All data in `/raw_data` will be preserved with a 2-year embargo, then made public."
    *   "All content in `/code` will be assigned a `BSD-3-Clause` license."
    *   "All files will be automatically checked against our TRAC-compliance validation suite upon ingestion."
2.  **Automated Ingestion:** The research team works as they normally would. The difference is that their tools are connected to the EverArchive object.
    *   The weather sensors on the roof automatically push their hourly readings via an API, which are ingested, timestamped, and validated.
    *   When a researcher commits new analysis code to the project's Git repository, a webhook fires, and the commit is automatically logged in the **Process Layer**.
    *   A satellite imagery dataset is uploaded. The system's **Automated QC** runs fixity checks and format validation. The **AI-Assisted Metadata** tool reads the GeoTIFF headers, extracts the geospatial coordinates and timestamps, and adds them as searchable metadata, saving a librarian hours of manual work.

**The Layers of a Scientific Project:**
*   **Surface Layer:** The final, peer-reviewed papers. The cleaned, validated, and documented public datasets.
*   **Process Layer:** The full, versioned Git history of the analysis software. The complete, raw, uncleaned sensor data. The logs from the supercomputer simulations. The full record of peer review comments on submitted papers.
*   **Core Layer:** The team's internal Slack channel for the project, where they debate hypotheses and interpret results. The grant proposal's "budget justification" section, which explains the economic constraints of the project. Early, failed simulation models.

---

### **Part 2: The Publishing Experience - The Living Document**

#### **The Publishing Workflow & The Schema Projector**
*   **The Steward's Action:** The research team needs a "project homepage" to share their progress with the NSF program officer and the public. Dr. Chen points a URL (`helios.university.edu`) to the Deep Authorship Package.
*   **The Schema Projector in Action:**
    *   **Live Project Dashboard:** The "Research Dashboard Projector" renders a real-time status page. It shows the latest published papers from the Surface Layer, pulls recent commit messages from the Process Layer to show development activity, and displays visualizations of the latest public data.
*   **The "Living Update" Experience:** The team publishes a new paper. They simply add the final PDF to the `/published_papers/` folder in the object's Surface Layer. The project website **updates automatically**, with the new paper appearing at the top of the list. No webmaster needed.
*   **The Value Proposition:** The project's public presence is a direct, live reflection of the actual research work, not a separate, manually-updated marketing site. This ensures transparency and dramatically reduces administrative overhead.

---

### **Part 3: The Archival Act - Defining a Permanent Legacy**

**The Archival Workflow:**
The five-year grant period ends. Dr. Chen's final task is to create the permanent, compliant archive of the entire project.

**The Permissions & Consent Configuration:**
*   **The Compliance Report Generation:** This is the killer feature for Dr. Chen. He clicks "Generate Compliance Report." The Schema Projector does the rest:
    1.  **NSF DMSP Report Projector:** It scans the entire object, finds all public datasets, lists their permanent identifiers, describes the preservation plan (by quoting the university's policy), and generates a perfect, pre-filled DMSP final report.
    2.  **TRAC Audit Trail Projector:** It generates a comprehensive report for the university's TRAC audit, providing an immutable, on-chain log of all fixity checks, format migrations, and access permissions changes for every file in the object.
*   **The Value Proposition:** What used to be a six-month, soul-crushing manual process of evidence gathering for Dr. Chen is now a **five-minute, automated task**. This is a revolutionary shift in institutional efficiency.

**The Finalization:**
The final object, with its compliance reports attached, is preserved.

---

### **Part 4: The Consumption Experience - A Multi-Faceted Afterlife**

#### **Consumption Scenario A: The Human Connection (The Skeptical Rival Researcher)**
*   **Consumer Persona:** A researcher from another university who is skeptical of the Helios Project's surprising results.
*   **The Discovery & Access:** She accesses the public Deep Authorship Package.
*   **The "Aha!" Moment:** She reads the final paper (Surface), but she doesn't trust the graphs. She clicks on a graph. The **"Reproducible Paper" Projector** opens. It shows her the exact slice of raw sensor data from the **Process Layer** that was used, the full Python script that generated the analysis, and a "Run in Cloud" button. She clicks it. A containerized environment spins up and re-runs the original analysis on the original data, perfectly reproducing the graph from the paper right before her eyes.
*   **The Value Proposition:** Her skepticism vanishes. The research is not just a claim; it is a verifiable, transparent, and reproducible fact. This new standard of trust accelerates scientific progress.

#### **Consumption Scenario B: The AI / Agentic Web Connection (The Meta-Analysis AI)**
*   **Consumer Persona:** A global climate modeling AI.
*   **The Task & Access:** The AI is tasked with synthesizing data from thousands of different climate studies to improve its model.
*   **The Synthesis & Insight:** The AI ingests the Helios object. It doesn't just read the conclusions in the paper. It goes directly to the **Process Layer** and ingests the terabytes of raw, high-resolution sensor dataâ€”data that was too large to be included in the traditional journal publication. It also reads the metadata about the sensors themselves, including their margin of error and calibration history.
*   **The Value Proposition:** By training on the clean, raw, and deeply contextualized data from hundreds of such objects, the AI can create a climate model with a far higher resolution and lower margin of error than was ever possible by using just the compressed, summarized data from published papers. The preserved Process Layer becomes the essential fuel for more accurate planetary-scale science.

---

### **Conclusion: The Value of the Complete Journey**
For Dr. Chen and the R1 University, EverArchive solves their biggest, most expensive problems: compliance, reporting, and manual bottlenecks. It transforms the archive from a costly, static repository into an efficient, automated, and living platform for transparent science. For the world, it ensures that the immense investment in scientific research is truly preserved, creating a permanent, verifiable, and deeply reusable foundation of knowledge for all future generations of humans and AIs.